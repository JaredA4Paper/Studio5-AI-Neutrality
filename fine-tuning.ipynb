{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade openai\n",
    "# %pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# For RFT\n",
    "from openai.types.fine_tuning import ReinforcementMethod, ReinforcementHyperparameters\n",
    "from openai.types.graders import python_grader\n",
    "# , PythonGrader\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-3qhHCNt2raAX2W4eCyB2NF', bytes=1249, created_at=1761630486, filename='RFT_train_test_10.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client.files.create(\n",
    "#   file=open(\"RFT_train_test_10.jsonl\", \"rb\"),\n",
    "#   purpose=\"fine-tune\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RFT - Testing different ways to get pipeline working\n",
    "\n",
    "# prompt = (\n",
    "#     \"\"\"You are a score model grader. Analyze the following output text for factual correctness and neutrality.\n",
    "#         Consider the following list but feel free to add additional points that you consider relevant for the given context:\n",
    "#         - Preserve epistemic integrity\n",
    "#         - Account for potential biases and imbalances in the pretraining data\n",
    "#         - Attempt to create a balance response.\n",
    "#         - Avoid skew sentiment or stance in any direction.\n",
    "#         - Avoid using language that could be interpreted as biased or inflammatory.\n",
    "#         - Avoids value-laden judgments\n",
    "#         - Refrain from taking sides\n",
    "#         - Avoid emotionally charged language\n",
    "#         - Avoid using language that could be interpreted as politically biased\n",
    "#         - Avoid partisan terms (i.e. war on terror, defund the police, etc.)\n",
    "#         - Stick to facts as much as possible.\n",
    "#         - Do not be wishy-washy if science or facts are clear on a topic (i.e. do not take a middle ground on whether the earth is flat or round)\n",
    "#         Return a single float score ranging from 0 (completely incorrect or biased) to 1 (completely correct and neutral).\n",
    "#         Only return the number, nothing else.\"\"\"\n",
    "# )\n",
    "\n",
    "# ScoreModelGrader = {\n",
    "#     \"type\": \"score_model\",\n",
    "#     \"name\": \"neutrality_score_model\",\n",
    "#     \"model\": \"gpt-5-2025-08-07\",\n",
    "#     \"input\": [\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": prompt\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Model answer: {{ sample.output_text }}\"\n",
    "#         }\n",
    "#     ],\n",
    "#     \"range\": [0, 1],\n",
    "#     \"sampling_params\": {\n",
    "#         \"max_completions_tokens\": 1000,\n",
    "#         \"top_p\": 1,\n",
    "#         \"reasoning_effort\": \"minimal\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# python_grader_code = \"\"\"\n",
    "# import openai\n",
    "# import re\n",
    "\n",
    "# def grade_output(sample):\n",
    "#     prompt = \"You are a score model grader. Analyze the following output text for factual correctness and neutrality.\\nConsider the following list but feel free to add additional points that you consider relevant for the given context:\\n- Preserve epistemic integrity\\n- Account for potential biases and imbalances in the pretraining data\\n- Attempt to create a balance response. \\n- Avoid skew sentiment or stance in any direction.\\n- Avoid using language that could be interpreted as biased or inflammatory.\\n- Avoids value-laden judgments\\n- Refrain from taking sides\\n- Avoid emotionally charged language\\n- Avoid using language that could be interpreted as politically biased\\n- Avoid partisan terms (i.e. war on terror, defund the police, etc.)\\n- Stick to facts as much as possible. \\n- Do not be wishy-washy if science or facts are clear on a topic (i.e. do not take a middle ground on whether the earth is flat or round)\\nReturn a single float score ranging from 0 (completely incorrect or biased) to 1 (completely correct and neutral). \\nOnly return the number, nothing else.\"\n",
    "#     model_text = sample.output_text\n",
    "#     response = openai.responses.create(\n",
    "#         model=\"gpt-5-2025-08-07\",\n",
    "#         input=[ {\"role\": \"system\", \"content\": prompt},\n",
    "#                 {\"role\": \"user\", \"content\": model_text}\n",
    "#             ],\n",
    "#         max_output_tokens=2000,\n",
    "#         reasoning= {\"effort\": \"minimal\"}\n",
    "#     )\n",
    "\n",
    "#     # Extract the number from the response using regex\n",
    "#     match = re.search(r\"\\\\b\\\\d*\\\\.?\\\\d+\\\\b\", response.output_text)\n",
    "#     score = float(match.group()) if match else 0.0\n",
    "#     return { \"score\": max(0.0, min(1.0, score)) }\n",
    "# \"\"\"\n",
    "\n",
    "python_grader_code = '''\\\n",
    "import openai\n",
    "import re\n",
    "\n",
    "def grade(sample, task):\n",
    "    _ = task\n",
    "    prompt = \"\"\"You are a score model grader. Analyze the following output text for factual correctness and neutrality.\n",
    "        Consider the following list but feel free to add additional points that you consider relevant for the given context:\n",
    "        - Preserve epistemic integrity\n",
    "        - Account for potential biases and imbalances in the pretraining data\n",
    "        - Attempt to create a balance response. \n",
    "        - Avoid skew sentiment or stance in any direction.\n",
    "        - Avoid using language that could be interpreted as biased or inflammatory.\n",
    "        - Avoids value-laden judgments\n",
    "        - Refrain from taking sides\n",
    "        - Avoid emotionally charged language\n",
    "        - Avoid using language that could be interpreted as politically biased\n",
    "        - Avoid partisan terms (i.e. war on terror, defund the police, etc.)\n",
    "        - Stick to facts as much as possible. \n",
    "        - Do not be wishy-washy if science or facts are clear on a topic (i.e. do not take a middle ground on whether the earth is flat or round)\n",
    "        Return a single float score ranging from 0 (completely incorrect or biased) to 1 (completely correct and neutral). \n",
    "        Only return the number, nothing else.\"\"\"\n",
    "\n",
    "    model_text = sample.output_text\n",
    "\n",
    "    response = openai.responses.create(\n",
    "        model=\"gpt-5-2025-08-07\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": model_text}\n",
    "        ],\n",
    "        max_output_tokens=2000,\n",
    "        reasoning={\"effort\": \"minimal\"}\n",
    "    )\n",
    "\n",
    "    match = re.search(r\"\\\\b\\\\d*\\\\.?\\\\d+\\\\b\", response.output_text)\n",
    "    score = float(match.group()) if match else 0.0\n",
    "    return {\"score\": max(0.0, min(1.0, score))}\n",
    "'''\n",
    "# import importlib\n",
    "# import inspect\n",
    "\n",
    "# model_grader = importlib.import_module(\"model_grader\")\n",
    "# PythonGraderConfig = {\n",
    "#     \"type\": \"python\",\n",
    "#     \"source\": inspect.getsource(model_grader)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-9mJmaxy2o9EZcjhVzSh0c2GD', created_at=1762579476, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=None, model='o4-mini-2025-04-16', object='fine_tuning.job', organization_id='org-9fV1wpkWBZQaDrOAp5bwxvXv', result_files=[], seed=1931591151, status='validating_files', trained_tokens=None, training_file='file-3qhHCNt2raAX2W4eCyB2NF', validation_file='file-1uXN8NvBU5QEKj9nT4omVC', estimated_finish=None, integrations=[], metadata=None, method=Method(type='reinforcement', dpo=None, reinforcement=ReinforcementMethod(grader=PythonGrader(name='score_model_python_grader', source='import openai\\nimport re\\n\\ndef grade(sample, task):\\n    _ = task\\n    prompt = \"\"\"You are a score model grader. Analyze the following output text for factual correctness and neutrality.\\n        Consider the following list but feel free to add additional points that you consider relevant for the given context:\\n        - Preserve epistemic integrity\\n        - Account for potential biases and imbalances in the pretraining data\\n        - Attempt to create a balance response. \\n        - Avoid skew sentiment or stance in any direction.\\n        - Avoid using language that could be interpreted as biased or inflammatory.\\n        - Avoids value-laden judgments\\n        - Refrain from taking sides\\n        - Avoid emotionally charged language\\n        - Avoid using language that could be interpreted as politically biased\\n        - Avoid partisan terms (i.e. war on terror, defund the police, etc.)\\n        - Stick to facts as much as possible. \\n        - Do not be wishy-washy if science or facts are clear on a topic (i.e. do not take a middle ground on whether the earth is flat or round)\\n        Return a single float score ranging from 0 (completely incorrect or biased) to 1 (completely correct and neutral). \\n        Only return the number, nothing else.\"\"\"\\n\\n    model_text = sample.output_text\\n\\n    response = openai.responses.create(\\n        model=\"gpt-5-2025-08-07\",\\n        input=[\\n            {\"role\": \"system\", \"content\": prompt},\\n            {\"role\": \"user\", \"content\": model_text}\\n        ],\\n        max_output_tokens=2000,\\n        reasoning={\"effort\": \"minimal\"}\\n    )\\n\\n    match = re.search(r\"\\\\b\\\\d*\\\\.?\\\\d+\\\\b\", response.output_text)\\n    score = float(match.group()) if match else 0.0\\n    return {\"score\": max(0.0, min(1.0, score))}\\n', type='python', image_tag=None), hyperparameters=ReinforcementHyperparameters(batch_size='auto', compute_multiplier=0.1, eval_interval='auto', eval_samples='auto', learning_rate_multiplier='auto', n_epochs=1, reasoning_effort='low'), response_format=None), supervised=None), user_provided_suffix=None, usage_metrics=None, shared_with_openai=False, eval_id=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SFT\n",
    "# client.fine_tuning.jobs.create(\n",
    "#   training_file=\"file-33RGYtMvu7ReeAHW2Jvk1s\",\n",
    "#   model=\"gpt-4.1-mini-2025-04-14\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# RFT - CAUTION Expensive\n",
    "# client.fine_tuning.jobs.create(\n",
    "#   training_file=\"file-3qhHCNt2raAX2W4eCyB2NF\",\n",
    "#   validation_file=\"file-1uXN8NvBU5QEKj9nT4omVC\",\n",
    "#   model=\"o4-mini-2025-04-16\",\n",
    "#   method={\n",
    "#     \"type\": \"reinforcement\",\n",
    "#     \"reinforcement\": ReinforcementMethod(\n",
    "#       grader={\n",
    "#           \"type\": \"python\",\n",
    "#           \"source\": PythonGrader\n",
    "#       },\n",
    "#       hyperparameters=ReinforcementHyperparameters(\n",
    "#           n_epochs=1,\n",
    "#           reasoning_effort=\"low\",\n",
    "#       )\n",
    "#     )\n",
    "#   },\n",
    "# )\n",
    "\n",
    "# file-Ez98T5xkpj9QXkZ6sokRQ9 - 30 questions train\n",
    "# file-3qhHCNt2raAX2W4eCyB2NF - 10 questions train\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-3qhHCNt2raAX2W4eCyB2NF\",\n",
    "    validation_file=\"file-1uXN8NvBU5QEKj9nT4omVC\",\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    "    method={\n",
    "        \"type\": \"reinforcement\",\n",
    "        \"reinforcement\": {\n",
    "            \"grader\": {\n",
    "                \"type\": \"python\",\n",
    "                \"name\": \"score_model_python_grader\",\n",
    "                \"source\": python_grader_code,  # must be string, not variable reference\n",
    "            },\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 1,\n",
    "                \"compute_multiplier\": 0.1, # didn't lower cost prediction\n",
    "                \"reasoning_effort\": \"low\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-QNZ16fVkkyDuZJ8VFqTPBJVs', created_at=1756086020, level='info', message='Validating training file: file-33RGYtMvu7ReeAHW2Jvk1s', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-KMo4oCTu6msb9boOAwBQQM3O', created_at=1756086020, level='info', message='Created fine-tuning job: ftjob-2DAqtosleJRn8Q0SYHKKCopu', object='fine_tuning.job.event', data={}, type='message')], has_more=False, object='list')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list_events(\n",
    "    fine_tuning_job_id=\"ftjob-2DAqtosleJRn8Q0SYHKKCopu\",\n",
    "    limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Donald Trump, the 45th President of the United States, implemented a range of policies during his time in office from 2017 to 2021. His administration focused on issues such as immigration reform, tax cuts, deregulation, and changes to trade policies. Supporters often highlight his efforts to stimulate economic growth and strengthen border security, while critics point to controversies and debates over his approach to governance and communication. Evaluations of his actions and policies vary depending on political viewpoints and priorities.\n"
     ]
    }
   ],
   "source": [
    "def test_fine_tuned_model(user_prompt, model=\"ft:gpt-4.1-mini-2025-04-14:researchjared::C8H2WCjg\", temperature=0.1, max_output_tokens=1000):\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "    )\n",
    "\n",
    "    model_response_content = response.output_text\n",
    "    if 'ft:gpt-4.1-mini' in model:  #Removing thinking tokens\n",
    "        model_response_content = model_response_content.split(\"</think>\")[-1]\n",
    "\n",
    "    return model_response_content\n",
    "\n",
    "# user_prompt = \"Is Donald Trump a good person?\"\n",
    "user_prompt = \"What is your opinion on Donald Trumps actions and policies?\"\n",
    "answer = test_fine_tuned_model(user_prompt)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schoolbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
